# ğŸ§  Memory-Powered MCP Chat with LangChain & OpenAI

This project implements an interactive memory-enabled conversational agent using LangChain's `ChatOpenAI`, a custom `MCPAgent`, and an MCP-compatible backend client defined in `mcp_use`. The assistant supports contextual conversation memory, configurable OpenAI models, and interactive terminal-based chatting.

---

## ğŸš€ Features

- âœ… Conversational memory using LangChain
- ğŸ¤– GPT-4o-mini (or any OpenAI model)
- ğŸ—ƒï¸ Loads config from `browser_mcp.json`
- ğŸ” Supports up to 15 reasoning steps
- âœ¨ Clear memory and exit options
- ğŸ” `.env` support for managing secrets

---

## ğŸ§© Project Structure

```
.
â”œâ”€â”€ browser_mcp.json         # Configuration file for MCPClient
â”œâ”€â”€ app.py                   # Main script for interactive chat
â”œâ”€â”€ .env                     # Contains OPENAI_API_KEY
â””â”€â”€ README.md                # Project documentation
```

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/Ajaytherala/mcpdemo.git
cd mcpdemo
```

### 2. Create and Configure `.env`

Create a `.env` file in the root directory and add your OpenAI API key:

```env
OPENAI_API_KEY=your_openai_api_key_here
```


---

## ğŸ§ª Run the Chat

```bash
uv run app.py
```

### In Chat:

- Type your message to start the conversation.
- Type `clear` to reset memory.
- Type `exit` or `quit` to leave the chat.

---

## ğŸ§  Customization

- **Model**: Change the model in the `ChatOpenAI` instance (default: `"gpt-4o-mini"`).
- **Memory Steps**: Modify `max_steps` in the `MCPAgent` initialization.
- **Client Config**: Update `browser_mcp.json` with your backend endpoints or connection details.


